{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJjK6piBILij"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90Tl80eLJ7rq"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train_2kmZucJ.csv\")\n",
    "test =  pd.read_csv(\"test_oJQbWVk.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Eg24D2uBLKpt",
    "outputId": "6abd6615-413b-4205-a6a4-7d1c99515b1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.744192\n",
       "1    0.255808\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u55De59RLSTi"
   },
   "outputs": [],
   "source": [
    "train['clean_tweet'] = train['tweet'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "test['clean_tweet'] = test['tweet'].apply(lambda x: re.sub(r'http\\S+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z5eb2yjILSmf"
   },
   "outputs": [],
   "source": [
    "punctuation = '!\"#$%&()*+-/:;<=>?@[\\\\]^_`{|}~'\n",
    "\n",
    "train['clean_tweet'] = train['clean_tweet'].apply(lambda x: ''.join(ch for ch in x if ch not in set(punctuation)))\n",
    "test['clean_tweet'] = test['clean_tweet'].apply(lambda x: ''.join(ch for ch in x if ch not in set(punctuation)))\n",
    "\n",
    "# convert text to lowercase\n",
    "train['clean_tweet'] = train['clean_tweet'].str.lower()\n",
    "test['clean_tweet'] = test['clean_tweet'].str.lower()\n",
    "\n",
    "# remove numbers\n",
    "train['clean_tweet'] = train['clean_tweet'].str.replace(\"[0-9]\", \" \")\n",
    "test['clean_tweet'] = test['clean_tweet'].str.replace(\"[0-9]\", \" \")\n",
    "\n",
    "# remove whitespaces\n",
    "train['clean_tweet'] = train['clean_tweet'].apply(lambda x:' '.join(x.split()))\n",
    "test['clean_tweet'] = test['clean_tweet'].apply(lambda x: ' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aaNTWPNKL3G6"
   },
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# function to lemmatize text\n",
    "def lemmatization(texts):\n",
    "    output = []\n",
    "    for i in texts:\n",
    "        s = [token.lemma_ for token in nlp(i)]\n",
    "        output.append(' '.join(s))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j0NDXmv6L3UZ"
   },
   "outputs": [],
   "source": [
    "train['clean_tweet'] = lemmatization(train['clean_tweet'])\n",
    "test['clean_tweet'] = lemmatization(test['clean_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wBUWqCf1L3aX"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>1435</td>\n",
       "      <td>0</td>\n",
       "      <td>Mommy - Daughter day! Wicked! Broadway fun! #w...</td>\n",
       "      <td>mommy daughter day wicked broadway fun wicked ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6269</th>\n",
       "      <td>6270</td>\n",
       "      <td>1</td>\n",
       "      <td>Why the fuck does it take so long for an Ipod ...</td>\n",
       "      <td>why the fuck do -PRON- take so long for an ipo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6312</th>\n",
       "      <td>6313</td>\n",
       "      <td>0</td>\n",
       "      <td>#2014 #New #Year by #Sony #A7 #2870OSS at #종각 ...</td>\n",
       "      <td>new year by sony a oss at 종각 보신각 제야의종 타종행사 seo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7255</th>\n",
       "      <td>7256</td>\n",
       "      <td>0</td>\n",
       "      <td>Cool girls for you! http://ow.ly/K7W8307ox4a #...</td>\n",
       "      <td>cool girl for -PRON- ff foodporn dj tokyo appl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4311</th>\n",
       "      <td>4312</td>\n",
       "      <td>0</td>\n",
       "      <td>Had the urge to game so I purchased a PS3 lol ...</td>\n",
       "      <td>have the urge to game so i purchase a ps lol e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2610</th>\n",
       "      <td>2611</td>\n",
       "      <td>0</td>\n",
       "      <td>An owner and his #dog #australianshepherd #bor...</td>\n",
       "      <td>an owner and -PRON- dog australianshepherd bor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6242</th>\n",
       "      <td>6243</td>\n",
       "      <td>0</td>\n",
       "      <td>I finally upgraded my phone! From iPhone 5se t...</td>\n",
       "      <td>i finally upgrade -PRON- phone from iphone se ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4757</th>\n",
       "      <td>4758</td>\n",
       "      <td>0</td>\n",
       "      <td>#Dell computers literally cause more stress th...</td>\n",
       "      <td>dell computer literally cause more stress then...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704</th>\n",
       "      <td>1705</td>\n",
       "      <td>0</td>\n",
       "      <td>he saw a cute guy and got happy! #william #gay...</td>\n",
       "      <td>-PRON- see a cute guy and get happy william ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2188</th>\n",
       "      <td>2189</td>\n",
       "      <td>1</td>\n",
       "      <td>Thank you Samsung, for the shottiest phone I'v...</td>\n",
       "      <td>thank -PRON- samsung , for the shotti phone -P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  label                                              tweet  \\\n",
       "1434  1435      0  Mommy - Daughter day! Wicked! Broadway fun! #w...   \n",
       "6269  6270      1  Why the fuck does it take so long for an Ipod ...   \n",
       "6312  6313      0  #2014 #New #Year by #Sony #A7 #2870OSS at #종각 ...   \n",
       "7255  7256      0  Cool girls for you! http://ow.ly/K7W8307ox4a #...   \n",
       "4311  4312      0  Had the urge to game so I purchased a PS3 lol ...   \n",
       "2610  2611      0  An owner and his #dog #australianshepherd #bor...   \n",
       "6242  6243      0  I finally upgraded my phone! From iPhone 5se t...   \n",
       "4757  4758      0  #Dell computers literally cause more stress th...   \n",
       "1704  1705      0  he saw a cute guy and got happy! #william #gay...   \n",
       "2188  2189      1  Thank you Samsung, for the shottiest phone I'v...   \n",
       "\n",
       "                                            clean_tweet  \n",
       "1434  mommy daughter day wicked broadway fun wicked ...  \n",
       "6269  why the fuck do -PRON- take so long for an ipo...  \n",
       "6312  new year by sony a oss at 종각 보신각 제야의종 타종행사 seo...  \n",
       "7255  cool girl for -PRON- ff foodporn dj tokyo appl...  \n",
       "4311  have the urge to game so i purchase a ps lol e...  \n",
       "2610  an owner and -PRON- dog australianshepherd bor...  \n",
       "6242  i finally upgrade -PRON- phone from iphone se ...  \n",
       "4757  dell computer literally cause more stress then...  \n",
       "1704  -PRON- see a cute guy and get happy william ga...  \n",
       "2188  thank -PRON- samsung , for the shotti phone -P...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SkCE110rL3et"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow>=1.7.0\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/14/e4538c2bc3ae9f4ce6f6ce7ef1180da05abc4a617afba798268232b01d0d/tensorflow-1.13.1-cp37-cp37m-win_amd64.whl (63.1MB)\n",
      "Collecting tensorboard<1.14.0,>=1.13.0 (from tensorflow>=1.7.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
      "Collecting gast>=0.2.0 (from tensorflow>=1.7.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Collecting absl-py>=0.1.6 (from tensorflow>=1.7.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/da/3f/9b0355080b81b15ba6a9ffcf1f5ea39e307a2778b2f2dc8694724e8abd5b/absl-py-0.7.1.tar.gz (99kB)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow>=1.7.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/2a/22/bd327063dd0bdf9d8d640b3185b760707842160e69df909db3fcaab5b758/grpcio-1.20.1-cp37-cp37m-win_amd64.whl (1.6MB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow>=1.7.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting protobuf>=3.6.1 (from tensorflow>=1.7.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/34/ef/f020691889031a8e1d8cb20711daa43cfe999e0768ff6903c4bf70c2eecd/protobuf-3.7.1-cp37-cp37m-win_amd64.whl (986kB)\n",
      "Collecting astor>=0.6.0 (from tensorflow>=1.7.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\n",
      "Collecting keras-applications>=1.0.6 (from tensorflow>=1.7.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/90/85/64c82949765cfb246bbdaf5aca2d55f400f792655927a017710a78445def/Keras_Applications-1.0.7-py2.py3-none-any.whl (51kB)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow>=1.7.0) (1.12.0)\n",
      "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow>=1.7.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow>=1.7.0) (1.16.2)\n",
      "Collecting keras-preprocessing>=1.0.5 (from tensorflow>=1.7.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/c0/bf/0315ef6a9fd3fc2346e85b0ff1f5f83ca17073f2c31ac719ab2e4da0d4a3/Keras_Preprocessing-1.0.9-py2.py3-none-any.whl (59kB)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow>=1.7.0) (0.33.1)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.14.0,>=1.13.0->tensorflow>=1.7.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/f5/e4/d8c18f2555add57ff21bf25af36d827145896a07607486cc79a2aea641af/Markdown-3.1-py2.py3-none-any.whl (87kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow>=1.7.0) (0.14.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from protobuf>=3.6.1->tensorflow>=1.7.0) (40.8.0)\n",
      "Requirement already satisfied: h5py in c:\\programdata\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow>=1.7.0) (2.9.0)\n",
      "Collecting mock>=2.0.0 (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow>=1.7.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: gast, absl-py, termcolor\n",
      "  Building wheel for gast (setup.py): started\n",
      "  Building wheel for gast (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Mehul\\AppData\\Local\\pip\\Cache\\wheels\\5c\\2e\\7e\\a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Building wheel for absl-py (setup.py): started\n",
      "  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Mehul\\AppData\\Local\\pip\\Cache\\wheels\\ee\\98\\38\\46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Mehul\\AppData\\Local\\pip\\Cache\\wheels\\7c\\06\\54\\bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "Successfully built gast absl-py termcolor\n",
      "Installing collected packages: absl-py, markdown, protobuf, grpcio, tensorboard, gast, termcolor, astor, keras-applications, mock, tensorflow-estimator, keras-preprocessing, tensorflow\n",
      "Successfully installed absl-py-0.7.1 astor-0.7.1 gast-0.2.2 grpcio-1.20.1 keras-applications-1.0.7 keras-preprocessing-1.0.9 markdown-3.1 mock-3.0.5 protobuf-3.7.1 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0 termcolor-1.1.0\n",
      "Collecting tensorflow-hub\n",
      "  Downloading https://files.pythonhosted.org/packages/10/5c/6f3698513cf1cd730a5ea66aec665d213adf9de59b34f362f270e0bd126f/tensorflow_hub-0.4.0-py2.py3-none-any.whl (75kB)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-hub) (1.12.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-hub) (3.7.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-hub) (1.16.2)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from protobuf>=3.4.0->tensorflow-hub) (40.8.0)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.4.0\n"
     ]
    }
   ],
   "source": [
    "! pip install \"tensorflow>=1.7.0\"\n",
    "! pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "XCdrw6juMNby",
    "outputId": "1779d0b0-4118-46f6-e974-76d09365a53a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0512 23:37:17.349824  5816 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n",
      "W0512 23:41:45.071908  5816 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AKCDcVfmMNhD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(1), Dimension(8), Dimension(1024)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [\"Roasted ants are a popular snack in Columbia\"]\n",
    "\n",
    "# Extract ELMo features \n",
    "embeddings = elmo(x, signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qdNAI3LmMNla"
   },
   "outputs": [],
   "source": [
    "def elmo_vectors(x):\n",
    "  embeddings = elmo(x.tolist(), signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    # return average of ELMo features\n",
    "    return sess.run(tf.reduce_mean(embeddings,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5yq_SxzvMNpA"
   },
   "outputs": [],
   "source": [
    "list_train = [train[i:i+100] for i in range(0,train.shape[0],100)]\n",
    "list_test = [test[i:i+100] for i in range(0,test.shape[0],100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xwbz_p0yM1zb"
   },
   "outputs": [],
   "source": [
    "# Extract ELMo embeddings\n",
    "elmo_train = [elmo_vectors(x['clean_tweet']) for x in list_train]\n",
    "elmo_test = [elmo_vectors(x['clean_tweet']) for x in list_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xAtZjnd0M16O"
   },
   "outputs": [],
   "source": [
    "elmo_train_new = np.concatenate(elmo_train, axis = 0)\n",
    "elmo_test_new = np.concatenate(elmo_test, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JKOe3W4UM1_b"
   },
   "outputs": [],
   "source": [
    "# save elmo_train_new\n",
    "pickle_out = open(\"train_elmo.pickle\",\"wb\")\n",
    "pickle.dump(elmo_train_new, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "# save elmo_test_new\n",
    "pickle_out = open(\"test_elmo.pickle\",\"wb\")\n",
    "pickle.dump(elmo_test_new, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "elXjzql7M2D4"
   },
   "outputs": [],
   "source": [
    "# load elmo_train_new\n",
    "pickle_in = open(\"train_elmo.pickle\", \"rb\")\n",
    "elmo_train_new = pickle.load(pickle_in)\n",
    "\n",
    "# load elmo_test_new\n",
    "pickle_in = open(\"test_elmo.pickle\", \"rb\")\n",
    "elmo_test_new = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fsx2rDx4M2I2"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(elmo_train_new, \n",
    "                                                  train['label'],  \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "S6YVVZQMM2P4",
    "outputId": "d3777dc5-784e-4140-c00b-df8f8d26b2de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "lreg = LogisticRegression()\n",
    "lreg.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1KuDJDI4NX6R"
   },
   "outputs": [],
   "source": [
    "preds_valid = lreg.predict(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JKWQ51JdNYEx",
    "outputId": "350e1dba-88d9-426c-9236-b7f2b8315eb0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7752675386444708"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(yvalid, preds_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZHgAGQQfNdmd"
   },
   "outputs": [],
   "source": [
    "# make predictions on test set\n",
    "preds_test = lreg.predict(elmo_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vq5Jor-QNhQb"
   },
   "outputs": [],
   "source": [
    "# prepare submission dataframe\n",
    "sub = pd.DataFrame({'id':test['id'], 'label':preds_test})\n",
    "\n",
    "# write predictions to a CSV file\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TP789_05Nhac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "symbolic link created for C:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\data\\en <<===>> C:\\ProgramData\\Anaconda3\\lib\\site-packages\\en_core_web_sm\n",
      "[+] Linking successful\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\en_core_web_sm -->\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\data\\en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of Sentiment _Analysis.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
